---
layout: post
title: reinforce_navigation
date: 2023-03-08
---


 
# A survey on Visual Navigation for Artificial Agents with deep reinforcement learning

the paper divide rl algorithms into three categories: 
1.Value-based methods  
2.Policy-based methods  
3.Actor-critic methods. 

 and list four most used network/algorithms:
 1.deep Q-network  
 2.Deep determinstic policy gradient  
 3.Asynchronous advantage actor-critic(A3C)  
 4.Proximal policy optimization  

 5 visual navigation methods:  
 1.direct drl vNavigation  
  In detail, image depth information is conducive for theagent to avoid obstacles, and the closed-loop detection can beused for efficient exploration and spatial reasoning.  
  
  A method is A3C with auxiliary tasks[59], but auxiliary task mainly depends on human selection. And then Zhu[63] fed the target images into actor-critic NN. In the area of path-planning, [69] proposed a differentiabe path planning method: value iteration network(VIN). [71] reconstructed VIN as arecursive convolutional network.
 2.hierarchical methods:
  To solve dimentional diaster, hierachical methods decomposes vnavigation into subproblems.    
  1) Hierarchical abstract machines  
take final gaol into some subgoals and solve subgoals first. But it relies on manually constructing subgoals.   
  ![]({{site.url}}/assets/images/ham.png)  
  then comes to [73]~[77]  
  2).option methods  
  big area!!  
 3.multi-task drl vNavigation
  Traditional DRL agents can navigate well in one domain but will perform poorly in other unseen domains, which meanstraditional DRL navigation lacks transferability.  
  1). distillation method  
  the original distillation model is the Rusuet al.[83]. Then convolution was introduced to solve the same data distribution problem

 2). progress model

 4.memory-inferene drl vNavigation
 1). replay buffer
 normal replay buffer in DQN  
 hierarchical prioritized experience replay to selectively choose experiences  
 store the world model in the buffer  
 SoRB graph search to decomposed target into a sequence of easier subgoals to solve sparse problem  
 2).memory networks  
 hese proposed architectures storerecent observations into their memory and retrieve relevantmemory based on the temporal context, common networks include MQN/RMQN/FRMQN
 3).episodic memory 

 5).vision-and-language (VLN)
 agents follow the natural languague instructions to navigate.

explore unseen environments by imitating its past and good decisions.

5.challenge:
1).data inefficiency
when inputs are high-dimensional, model usually needs a large number of samples./ sparse rewards-> poor convergence and long training time.
2).poor generalization
 deep network are hard to tranfer
     ) partial observable
6.oppotunities
1).policy hierarchy
decomposes into subproblems
2).meta learning
training from a small amount of data
3).memory
 draw on previous experience in similar conditions from memory
4).multi-modal fusion


### Target-driven Visual Navigation in Indoor Scenesusing Deep Reinforcement Learning  
this paper proposes two methods for solving two problems:1.lack of generalization capability to new goals 2.data inefficiency, require several episodes to converge.
Then comes to two solutions:1.actor-critic models include targets. 2. propose AI2-THOR framework provides episodes with high-quality 3D scene.

However,general DRL approaches (e.g., [2], [3]) are designed tolearn a policy that depends only on the current state, andthe goal is implicitly embedded in the model parameters.Hence, it is necessary to learn new model parameters for anew target. This is problematic since training DRL agents iscomputationally expensive.

1.target generalization 2.scene generalization 3.real-world generalization


### Value Iteration Networks  
novel differentiable approximation of the value-iteration algorithm 


### End-to-End Training of Deep Visuomotor Policies  
