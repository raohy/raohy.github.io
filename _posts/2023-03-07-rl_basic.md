---
layout: post
title: reinforce_basic
date: 2023-03-07
---

<!--

# {{ page.title }}
 
## ç›®å½•
+ [ç¬¬ä¸€éƒ¨åˆ†](#partI)
+ [ç¬¬äºŒéƒ¨åˆ†](#partII)
+ [ç¬¬ä¸‰éƒ¨åˆ†](#partIII)
 {{ page.date|date_to_string }}
----------------------------------
 -->

# A3C

# A2C
add one coordinator to 

# DPG
some notes:
1. deterministic policy gradient is the limitting case, as policy variance tends to zero.
2. policy gradient integrates over both state and action spaces, whereas in the determinstic case it only integrates over the state space.
3. build a actor-critic algorithms

action space A and policy needs to be continuous
Q:
ä¸ºä»€ä¹ˆ approximation é‚£é‡Œi)è¦æ±‚çº¿æ€§

# MADDPG
centralizd training(critic,allowing the policy to use extra information to ease training )/decentralized execution(actor)
thus proposed a simple extension of actor-critic policy gradient methods where the critic is augmented with extra information about the policy of the other agents

for training:
    


$ E=mc^2 $
$$ \beta $$

# TRPO
 trust region policy optimization(TRPO).   
 Normal policy gradient method suffers from the unconvergence because of the big difference one batch. The paper prove that minimizing a certain surrogate objective funtion guarantees policy improvement with non-trivial step sizes.

###Approximation:
$Î·(Ï€) =Es0,a0,...[âˆâˆ‘t=0Î³tr(st)]$ Î·(Ï€)is the reward function that starting from the initial state.
the expected return of another policy ÌƒÏ€ can be expressed as
Î·( ÌƒÏ€) =Î·(Ï€) +Es0,a0,Â·Â·Â·âˆ¼ ÌƒÏ€[âˆâˆ‘t=0Î³tAÏ€(st,at)]
define LÏ€( ÌƒÏ€) =Î·(Ï€) +âˆ‘sÏÏ€(s)âˆ‘a ÌƒÏ€(a|s)AÏ€(s,a).
note that the visitation frequency Ï ÌƒÏ€(s) is approximated as ÏÏ€(s). It leads to some kinds of error.
the paper derive the lower bound 
Î·(Ï€new)â‰¥LÏ€old(Ï€new)âˆ’2Î³(1âˆ’Î³)2Î±2where= maxsâˆ£âˆ£Eaâˆ¼Ï€â€²(a|s)[AÏ€(s,a)]âˆ£âˆ£ by KL divergence.

# PPO
the gradient estimator is g=Ë†Et[âˆ‡Î¸logÏ€Î¸(at|st)Ë†At]
it is equal to differential the loss function:
LPG(Î¸) =Ë†Et[logÏ€Î¸(at|st)Ë†At]
