---
layout: post
title: reinforce_basic
date: 2023-03-07
---

<!--

# {{ page.title }}
 
## 目录
+ [第一部分](#partI)
+ [第二部分](#partII)
+ [第三部分](#partIII)
 {{ page.date|date_to_string }}
----------------------------------
 -->

# A3C

# A2C
add one coordinator to 

# DPG
some notes:
1. deterministic policy gradient is the limitting case, as policy variance tends to zero.
2. policy gradient integrates over both state and action spaces, whereas in the determinstic case it only integrates over the state space.
3. build a actor-critic algorithms

action space A and policy needs to be continuous
Q:
为什么 approximation 那里i)要求线性

# MADDPG
centralizd training(critic,allowing the policy to use extra information to ease training )/decentralized execution(actor)
thus proposed a simple extension of actor-critic policy gradient methods where the critic is augmented with extra information about the policy of the other agents

for training:
    


$ E=mc^2 $
$$ \beta $$

# TRPO
 trust region policy optimization(TRPO).   
 Normal policy gradient method suffers from the unconvergence because of the big difference one batch. The paper prove that minimizing a certain surrogate objective funtion guarantees policy improvement with non-trivial step sizes.

###Approximation:
$η(π) =Es0,a0,...[∞∑t=0γtr(st)]$ η(π)is the reward function that starting from the initial state.
the expected return of another policy ̃π can be expressed as
η( ̃π) =η(π) +Es0,a0,···∼ ̃π[∞∑t=0γtAπ(st,at)]
define Lπ( ̃π) =η(π) +∑sρπ(s)∑a ̃π(a|s)Aπ(s,a).
note that the visitation frequency ρ ̃π(s) is approximated as ρπ(s). It leads to some kinds of error.
the paper derive the lower bound 
η(πnew)≥Lπold(πnew)−2γ(1−γ)2α2where= maxs∣∣Ea∼π′(a|s)[Aπ(s,a)]∣∣ by KL divergence.

# PPO
the gradient estimator is g=ˆEt[∇θlogπθ(at|st)ˆAt]
it is equal to differential the loss function:
LPG(θ) =ˆEt[logπθ(at|st)ˆAt]
