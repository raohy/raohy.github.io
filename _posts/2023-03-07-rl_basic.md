---
layout: post
title: reinforce_basic
date: 2023-03-07
---

<!--

# {{ page.title }}
 
## ç›®å½•
+ [ç¬¬ä¸€éƒ¨åˆ†](#partI)
+ [ç¬¬äºŒéƒ¨åˆ†](#partII)
+ [ç¬¬ä¸‰éƒ¨åˆ†](#partIII)
 {{ page.date|date_to_string }}
----------------------------------
 -->

# TRPO
 trust region policy optimization(TRPO).   
 Normal policy gradient method suffers from the unconvergence because of the big difference one batch. The paper prove that minimizing a certain surrogate objective funtion guarantees policy improvement with non-trivial step sizes.

###Approximation:
Î·(Ï€) =Es0,a0,...[âˆâˆ‘t=0Î³tr(st)] Î·(Ï€)is the reward function that starting from the initial state.
the expected return of another policy ÌƒÏ€ can be expressed as
Î·( ÌƒÏ€) =Î·(Ï€) +Es0,a0,Â·Â·Â·âˆ¼ ÌƒÏ€[âˆâˆ‘t=0Î³tAÏ€(st,at)]
define LÏ€( ÌƒÏ€) =Î·(Ï€) +âˆ‘sÏÏ€(s)âˆ‘a ÌƒÏ€(a|s)AÏ€(s,a).
note that the visitation frequency Ï ÌƒÏ€(s) is approximated as ÏÏ€(s). It leads to some kinds of error.
the paper derive the lower bound 
Î·(Ï€new)â‰¥LÏ€old(Ï€new)âˆ’2Î³(1âˆ’Î³)2Î±2where= maxsâˆ£âˆ£Eaâˆ¼Ï€â€²(a|s)[AÏ€(s,a)]âˆ£âˆ£ by KL divergence.

# PPO
the gradient estimator is g=Ë†Et[âˆ‡Î¸logÏ€Î¸(at|st)Ë†At]
it is equal to differential the loss function:
LPG(Î¸) =Ë†Et[logÏ€Î¸(at|st)Ë†At]
