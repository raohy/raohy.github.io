---
layout: post
title: reinforce_basic
date: 2023-03-07
---

<!--

# {{ page.title }}
 
## 目录
+ [第一部分](#partI)
+ [第二部分](#partII)
+ [第三部分](#partIII)
 {{ page.date|date_to_string }}
----------------------------------
 -->

# TRPO
 trust region policy optimization(TRPO).   
 Normal policy gradient method suffers from the unconvergence because of the big difference one batch. The paper prove that minimizing a certain surrogate objective funtion guarantees policy improvement with non-trivial step sizes.

###Approximation:
η(π) =Es0,a0,...[∞∑t=0γtr(st)] η(π)is the reward function that starting from the initial state.
the expected return of another policy ̃π can be expressed as
η( ̃π) =η(π) +Es0,a0,···∼ ̃π[∞∑t=0γtAπ(st,at)]
define Lπ( ̃π) =η(π) +∑sρπ(s)∑a ̃π(a|s)Aπ(s,a).
note that the visitation frequency ρ ̃π(s) is approximated as ρπ(s). It leads to some kinds of error.
the paper derive the lower bound 
η(πnew)≥Lπold(πnew)−2γ(1−γ)2α2where= maxs∣∣Ea∼π′(a|s)[Aπ(s,a)]∣∣ by KL divergence.

# PPO
the gradient estimator is g=ˆEt[∇θlogπθ(at|st)ˆAt]
it is equal to differential the loss function:
LPG(θ) =ˆEt[logπθ(at|st)ˆAt]
