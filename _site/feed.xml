<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.3">Jekyll</generator><link href="http://localhost:5555/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:5555/" rel="alternate" type="text/html" /><updated>2023-03-09T16:56:48+08:00</updated><id>http://localhost:5555/feed.xml</id><title type="html">rao</title><subtitle>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.</subtitle><entry><title type="html">Welcome to Rao’s blog</title><link href="http://localhost:5555/blog/welcome-to-jekyll/" rel="alternate" type="text/html" title="Welcome to Rao’s blog" /><published>2023-03-08T10:59:04+08:00</published><updated>2023-03-08T10:59:04+08:00</updated><id>http://localhost:5555/blog/welcome-to-jekyll</id><content type="html" xml:base="http://localhost:5555/blog/welcome-to-jekyll/">&lt;p&gt;You’ll find this post in your &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_posts&lt;/code&gt; directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jekyll serve&lt;/code&gt;, which launches a web server and auto-regenerates your site when a file is updated.&lt;/p&gt;

&lt;p&gt;To add new posts, simply add a file in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_posts&lt;/code&gt; directory that follows the convention &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;YYYY-MM-DD-name-of-post.ext&lt;/code&gt; and includes the necessary front matter. Take a look at the source for this post to get an idea about how it works.&lt;/p&gt;

&lt;p&gt;Jekyll also offers powerful support for code snippets:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;puts&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Hi, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;#{&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&apos;Tom&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#=&amp;gt; prints &apos;Hi, Tom&apos; to STDOUT.&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Check out the &lt;a href=&quot;https://jekyllrb.com/docs/home&quot;&gt;Jekyll docs&lt;/a&gt; for more info on how to get the most out of Jekyll. File all bugs/feature requests at &lt;a href=&quot;https://github.com/jekyll/jekyll&quot;&gt;Jekyll’s GitHub repo&lt;/a&gt;. If you have questions, you can ask them on &lt;a href=&quot;https://talk.jekyllrb.com/&quot;&gt;Jekyll Talk&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html">You’ll find this post in your _posts directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run jekyll serve, which launches a web server and auto-regenerates your site when a file is updated.</summary></entry><entry><title type="html">reinforce_navigation</title><link href="http://localhost:5555/blog/rl-vnavi/" rel="alternate" type="text/html" title="reinforce_navigation" /><published>2023-03-08T00:00:00+08:00</published><updated>2023-03-08T00:00:00+08:00</updated><id>http://localhost:5555/blog/rl-vnavi</id><content type="html" xml:base="http://localhost:5555/blog/rl-vnavi/">&lt;h1 id=&quot;a-survey-on-visual-navigation-for-artificial-agents-with-deep-reinforcement-learning&quot;&gt;A survey on Visual Navigation for Artificial Agents with deep reinforcement learning&lt;/h1&gt;

&lt;p&gt;the paper divide rl algorithms into three categories: 
1.Value-based methods
2.Policy-based methods
3.Actor-critic methods.&lt;/p&gt;

&lt;p&gt;and list four most used network/algorithms:
 1.deep Q-network
 2.Deep determinstic policy gradient
 3.Asynchronous advantage actor-critic(A3C)
 4.Proximal policy optimization&lt;/p&gt;

&lt;p&gt;5 visual navigation methods:
 1.direct drl vNavigation
  In detail, image depth information is conducive for theagent to avoid obstacles, and the closed-loop detection can beused for efficient exploration and spatial reasoning.&lt;/p&gt;

&lt;p&gt;A method is A3C with auxiliary tasks[59], but auxiliary task mainly depends on human selection. And then Zhu[63] fed the target images into actor-critic NN. In the area of path-planning, [69] proposed a differentiabe path planning method: value iteration network(VIN). [71] reconstructed VIN as arecursive convolutional network.
 2.hierarchical methods:
  To solve dimentional diaster, hierachical methods decomposes vnavigation into subproblems.
  1) Hierarchical abstract machines&lt;/p&gt;</content><author><name></name></author><summary type="html">A survey on Visual Navigation for Artificial Agents with deep reinforcement learning</summary></entry><entry><title type="html">reinforce_basic</title><link href="http://localhost:5555/blog/rl_basic/" rel="alternate" type="text/html" title="reinforce_basic" /><published>2023-03-07T00:00:00+08:00</published><updated>2023-03-07T00:00:00+08:00</updated><id>http://localhost:5555/blog/rl_basic</id><content type="html" xml:base="http://localhost:5555/blog/rl_basic/">&lt;!--

# reinforce_basic
 
## 目录
+ [第一部分](#partI)
+ [第二部分](#partII)
+ [第三部分](#partIII)
 07 Mar 2023
----------------------------------
 --&gt;
&lt;h2 id=&quot;第一部分&quot;&gt;第一部分&lt;/h2&gt;

&lt;h1 id=&quot;trpo&quot;&gt;TRPO&lt;/h1&gt;
&lt;p&gt;trust region policy optimization(TRPO). The paper prove that minimizing a certain surrogate objective funtion guarantees policy improvement with non-trivial step sizes.&lt;/p&gt;

&lt;p&gt;###Approximation:
η(π) =Es0,a0,…[∞∑t=0γtr(st)] η(π)is the reward function that starting from the initial state.
the expected return of another policy ̃π can be expressed as
η( ̃π) =η(π) +Es0,a0,···∼ ̃π[∞∑t=0γtAπ(st,at)]
define Lπ( ̃π) =η(π) +∑sρπ(s)∑a ̃π(a|s)Aπ(s,a).
note that the visitation frequency ρ ̃π(s) is approximated as ρπ(s). It leads to some kinds of error.
the paper derive the lower bound 
η(πnew)≥Lπold(πnew)−2γ(1−γ)2α2where= maxs∣∣Ea∼π′(a|s)[Aπ(s,a)]∣∣ by KL divergence.&lt;/p&gt;

&lt;h1 id=&quot;ppo&quot;&gt;PPO&lt;/h1&gt;
&lt;p&gt;the gradient estimator is g=ˆEt[∇θlogπθ(at|st)ˆAt]
it is equal to differential the loss function:
LPG(θ) =ˆEt[logπθ(at|st)ˆAt]&lt;/p&gt;</content><author><name></name></author><summary type="html">&amp;lt;!–</summary></entry></feed>