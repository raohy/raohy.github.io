<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.3">Jekyll</generator><link href="http://localhost:5555/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:5555/" rel="alternate" type="text/html" /><updated>2023-06-20T14:18:48+08:00</updated><id>http://localhost:5555/feed.xml</id><title type="html">rao</title><subtitle>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.</subtitle><entry><title type="html">éšç§è®¡ç®—</title><link href="http://localhost:5555/blog/%E9%9A%90%E7%A7%81%E8%AE%A1%E7%AE%97/" rel="alternate" type="text/html" title="éšç§è®¡ç®—" /><published>2023-06-07T00:00:00+08:00</published><updated>2023-06-07T00:00:00+08:00</updated><id>http://localhost:5555/blog/%E9%9A%90%E7%A7%81%E8%AE%A1%E7%AE%97</id><content type="html" xml:base="http://localhost:5555/blog/%E9%9A%90%E7%A7%81%E8%AE%A1%E7%AE%97/">&lt;p&gt;éšç§è®¡ç®—&lt;/p&gt;

&lt;p&gt;1.å®‰å…¨å¤šæ–¹è®¡ç®—ï¼ˆMPCï¼‰
å¤šæ–¹è®¡ç®—ï¼šå°†å¤šä¸ªå‚ä¸è€…å„è‡ªçš„æ•°æ®å‡‘åœ¨ä¸€èµ·ï¼Œå¹¶åœ¨è¿™ä¸ªå¤§æ•°æ®ä¸Šè¿›è¡Œä¸€å®šçš„è®¡ç®—ï¼Œä¾‹å¦‚æœ‰æ•°æ®é›†ï¼ša1,a2,a3..an, ç›®çš„ï¼šå¾—åˆ°æŸä¸ªå‡½æ•°è¾“å‡ºï¼šf(a1.a2,â€¦,an)
å®‰å…¨å¤šæ–¹è®¡ç®—ï¼ˆSecure Multi-party Computation,MPC)èƒ½å¤Ÿä½¿å¤šæ–¹åœ¨ä¸çŸ¥æ™“å¯¹æ–¹å†…å®¹çš„æƒ…å†µä¸‹ï¼Œå‚ä¸ååŒè®¡ç®—ï¼Œæœ€ç»ˆäº§ç”Ÿæœ‰ä»·å€¼çš„åˆ†æå†…å®¹ã€‚å³å¯¹é™¤äº†æ•°æ®æºï¼Œå¯¹å…¶ä»–æ•°æ®æ–¹å’Œä¸­å¤®å¤„ç†æ–¹éƒ½ä¸å¯è§ã€‚ä¾èµ–äºå¤šç§å¯†ç å­¦åŸºç¡€ä¸Šçš„å·¥å…·åº”ç”¨ã€‚
å¸¸è§çš„å®‰å…¨è®¡ç®—æ–¹æ³•å¯ä»¥åˆ†ä¸ºä¸¤ç±»ï¼šåŸºäºå™ªéŸ³å’ŒéåŸºäºå™ªéŸ³çš„ã€‚
åŸºäºå™ªéŸ³çš„,ä»£è¡¨æ˜¯ï¼šå·®åˆ†éšç§ç®—æ³•ã€‚å¯¹æ•°æ®ç”¨å™ªå£°å¹²æ‰°ï¼Œç±»ä¼¼äºå›¾ç‰‡æ‰“é©¬èµ›å…‹ã€‚ä¼˜ç‚¹ï¼šæ•ˆç‡é«˜ï¼Œä½†æœ€åå¾—åˆ°çš„ç»“æœä¸å¤Ÿå‡†ç¡®ï¼Œä¼šæœ‰è¯¯å·®ã€‚
éå™ªå£°æ–¹æ³•ï¼šä¸»è¦æœ‰æ··æ·†ç”µè·¯(Garbled Circuit).åŒæ€åŠ å¯†(Homomorphic Encryption)/å¯†é’¥åˆ†äº«(Secret Sharing)
æ··æ·†ç”µè·¯ï¼š
 ç†è®ºåŸºç¡€ï¼šä»»ä½•å‡½æ•°éƒ½å¯ä»¥è¢«ç”µè·¯æ‰€è¡¨ç¤ºï¼Œä¾‹å¦‚æœ‰x,yæƒ³è¿›è¡Œè¿ç®—ï¼Œå³å¾—åˆ°ç»“æœf(x,y)=z
 Aæ ¹æ®å‡½æ•°ç”Ÿæˆç”µè·¯ï¼Œè®¡ç®—å¾—åˆ°ç”µè·¯çš„çœŸå€¼è¡¨ï¼Œå¯¹è¾“å…¥x,yè¿›è¡Œæ˜ å°„ï¼Œå¾—åˆ°å››ä¸ªä¸åŒçš„æ ‡è¯†ï¼ˆx/y=1/0)ï¼Œç”¨æ ‡è¯†å¯¹ç»“æœè¿›è¡Œå¯¹ç§°åŠ å¯†ã€‚å°†åŠ å¯†å¾—åˆ°çš„æ··æ·†è¡¨ï¼ˆå³å¤šä¸ªåŠ å¯†çš„ç»“æœå€¼ï¼‰ä¼ è¾“ç»™B.
 Aå°†aè¾“å…¥çš„æ˜ å°„ä¼ è¾“ç»™Bï¼ŒBé€šè¿‡ä¸ç»æ„ä¼ è¾“å¾—åˆ°å…¶è¾“å…¥å¯¹åº”çš„æ ‡è¯†ï¼Œå†å¯¹æ··æ·†è¡¨è¿›è¡Œè§£ç æ“ä½œï¼Œå°†ç»“æœå›ä¼ ã€‚
å¯†é’¥åˆ†äº«ï¼š
èƒŒæ™¯ï¼šï¼ˆt-nï¼‰ï¼Œå³nä¸ªç”¨æˆ·ï¼Œå¦‚æœæœ‰å¤§äºç­‰äºtä¸ªç”¨æˆ·çš„ä¿¡æ¯ï¼Œå³å¯è¿˜åŸå‡ºåŸä¿¡æ¯
shamirå¤šé¡¹å¼æ³•ï¼š
&lt;img src=&quot;http://localhost:5555/assets/images/éšç§è®¡ç®—/shamir.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;http://localhost:5555/assets/images/éšç§è®¡ç®—/shamir2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;æ¯”ç‰¹åˆ†äº«ï¼š&lt;/p&gt;

&lt;p&gt;â‘  å·®åˆ†éšç§
åè¯ï¼šéšæœºåŒ–ç®—æ³•Aï¼ˆèƒ½å¤Ÿè¾“å‡ºæŸä¸€åˆ†å¸ƒçš„ç®—æ³•ï¼‰ï¼Œç›¸é‚»æ•°æ®é›†
&lt;img src=&quot;http://localhost:5555/assets/images/éšç§è®¡ç®—/å·®åˆ†éšç§.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;
è¯¥ç®—æ³•Aä½œç”¨äºä»»ä½•ç›¸é‚»æ•°æ®é›†ï¼Œå¾—åˆ°ä¸€ä¸ªç‰¹å®šè¾“å‡ºOçš„æ¦‚ç‡å·®ä¸å¤šã€‚è§‚å¯Ÿè€…é€šè¿‡è§‚å¯Ÿè¾“å‡ºç»“æœå¾ˆéš¾å¯Ÿè§‰å‡ºæ•°æ®é›†çš„å¾®å°å˜åŒ–ã€‚
æœ€ç®€å•çš„æ–¹æ³•ï¼šåŠ ç™½å™ªå£°ï¼ˆæ‹‰æ™®æ‹‰æ–¯å™ªå£°/é«˜æ–¯å™ªå£°ï¼‰
å‚è€ƒï¼šhttps://zhuanlan.zhihu.com/p/31635977
2.è”é‚¦å­¦ä¹ 
åœ¨æœ¬åœ°å­¦ä¹ åˆ°æ¨¡å‹ï¼Œå°†æ¨¡å‹æ•°æ®é€šè¿‡åŠ å¯†ç®—æ³•ä¼ è¾“åˆ°ä¸­å¿ƒèŠ‚ç‚¹ï¼Œè¿›è¡ŒäºŒæ¬¡è®­ç»ƒåå¾—åˆ°æœ€ç»ˆçš„è®­ç»ƒæ¨¡å‹&lt;/p&gt;</content><author><name></name></author><summary type="html">éšç§è®¡ç®—</summary></entry><entry><title type="html">reinforce_navigation</title><link href="http://localhost:5555/blog/rl-lux/" rel="alternate" type="text/html" title="reinforce_navigation" /><published>2023-06-01T00:00:00+08:00</published><updated>2023-06-01T00:00:00+08:00</updated><id>http://localhost:5555/blog/rl-lux</id><content type="html" xml:base="http://localhost:5555/blog/rl-lux/">&lt;h2 id=&quot;approch-summary&quot;&gt;Approch Summary&lt;/h2&gt;
&lt;h3 id=&quot;top1&quot;&gt;top1&lt;/h3&gt;</content><author><name></name></author><summary type="html">Approch Summary top1</summary></entry><entry><title type="html">principal of economics</title><link href="http://localhost:5555/blog/p_o_economics/" rel="alternate" type="text/html" title="principal of economics" /><published>2023-04-24T00:00:00+08:00</published><updated>2023-04-24T00:00:00+08:00</updated><id>http://localhost:5555/blog/p_o_economics</id><content type="html" xml:base="http://localhost:5555/blog/p_o_economics/">&lt;h3 id=&quot;principal-of-economics&quot;&gt;Principal of Economics&lt;/h3&gt;
&lt;h4 id=&quot;ç¬¬ä¸€ç¯‡-å¯¼è¨€&quot;&gt;ç¬¬ä¸€ç¯‡ å¯¼è¨€&lt;/h4&gt;
&lt;h5 id=&quot;ç»æµå­¦åå¤§åŸç†&quot;&gt;ç»æµå­¦åå¤§åŸç†&lt;/h5&gt;
&lt;ol&gt;
  &lt;li&gt;äººä»¬é¢ä¸´æƒè¡¡å–èˆ&lt;/li&gt;
  &lt;li&gt;æŸç§ä¸œè¥¿çš„æˆæœ¬æ˜¯ä¸ºäº†å¾—åˆ°å®ƒæ‰€æ”¾å¼ƒçš„ä¸œè¥¿&lt;/li&gt;
  &lt;li&gt;ç†æ€§äººè€ƒè™‘è¾¹é™…é‡&lt;/li&gt;
  &lt;li&gt;äººä»¬ä¼šå¯¹æ¿€åŠ±åšå‡ºååº”&lt;/li&gt;
  &lt;li&gt;è´¸æ˜“å¯ä»¥ä½¿æ¯ä¸ªäººçš„çŠ¶å†µéƒ½å˜å¾—æ›´å¥½&lt;/li&gt;
  &lt;li&gt;å¸‚åœºé€šå¸¸æ˜¯ç»„ç»‡ç»æµæ´»åŠ¨çš„ä¸€ç§å¥½æ–¹æ³•&lt;/li&gt;
  &lt;li&gt;æ”¿åºœæœ‰æ—¶å¯ä»¥æ”¹å–„å¸‚åœºç»“æœ&lt;/li&gt;
  &lt;li&gt;ä¸€å›½çš„ç”Ÿæ´»æ°´å¹³å–å†³äºå®ƒç”Ÿäº§ç‰©å“ä¸æœåŠ¡çš„èƒ½åŠ›&lt;/li&gt;
  &lt;li&gt;å½“æ”¿åºœå‘å‘è¡Œäº†è¿‡å¤šè´§å¸æ—¶ï¼Œç‰©ä»·ä¸Šå‡&lt;/li&gt;
  &lt;li&gt;ç¤¾ä¼šé¢ä¸´é€šè´§è†¨èƒ€ä¸å¤±ä¸šä¹‹é—´çš„çŸ­æœŸæƒè¡¡å–èˆ&lt;/li&gt;
&lt;/ol&gt;

&lt;h5 id=&quot;åƒç»æµå­¦å®¶ä¸€æ ·æ€è€ƒ&quot;&gt;åƒç»æµå­¦å®¶ä¸€æ ·æ€è€ƒ&lt;/h5&gt;
&lt;p&gt;ä½œä¸ºç§‘å­¦å®¶çš„ç»æµå­¦å®¶
ç§‘å­¦æ–¹æ³•ï¼šè§‚å¯Ÿ/ç†è®ºå’Œè¿›ä¸€æ­¥è§‚å¯Ÿ-&amp;gt;å‡è®¾-&amp;gt;å¾—å‡ºç»æµæ¨¡å‹ï¼š
eg. 
&lt;img src=&quot;http://localhost:5555/assets/images/p_o_economics/circular_flow.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;
å¾ªç¯æµé‡å›¾
&lt;img src=&quot;http://localhost:5555/assets/images/p_o_economics/production_pssibilities_front.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;
ç”Ÿäº§å¯èƒ½æ€§è¾¹ç•Œ&lt;/p&gt;

&lt;p&gt;å¾®è§‚ç»æµå­¦ï¼šç ”ç©¶å®¶åº­å’Œä¼ä¸šå¦‚ä½•åšå‡ºå†³ç­–ï¼Œä»¥åŠå®ƒä»¬å¦‚ä½•åœ¨ç‰¹å®šå¸‚åœºä¸Šç›¸äº’äº¤æ˜“
å®è§‚ç»æµå­¦ï¼šç ”ç©¶æ•´ä½“ç»æµç°è±¡&lt;/p&gt;

&lt;p&gt;ä½œä¸ºæ”¿ç­–é¡¾é—®çš„ç»æµå­¦å®¶
å®è¯è¡¨è¿°ï¼šæè¿°æ€§çš„ï¼šæœ€ä½å·¥èµ„æ³•å¼•èµ·äº†å¤±ä¸š
è§„èŒƒè¡¨è¿°ï¼šè§„å®šæ€§çš„ï¼šæ”¿åºœåº”è¯¥æé«˜æœ€ä½å·¥èµ„&lt;/p&gt;

&lt;h5 id=&quot;ç›¸äº’ä¾å­˜æ€§ä¸è´¸æ˜“çš„å¥½å¤„&quot;&gt;ç›¸äº’ä¾å­˜æ€§ä¸è´¸æ˜“çš„å¥½å¤„&lt;/h5&gt;

&lt;p&gt;å‡è®¾Frankå’ŒRubyæ¯äººæ¯å¤©å·¥ä½œ8å°æ—¶ï¼ŒæŠŠè¿™ä¸ªæ—¶é—´ç”¨äºç”Ÿäº§åœŸè±†å’Œç‰›è‚‰ä¸Š
        â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”-
        ç”Ÿäº§1ç›å¸æ‰€éœ€è¦çš„æ—¶é—´       8ä¸ªå°æ—¶çš„äº§é‡ï¼ˆç›å¸ï¼‰
        â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”-
        ç‰›è‚‰       åœŸè±†             ç‰›è‚‰     åœŸè±†
        â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€“
Frank   60          15              8       32
Ruby    20          10              24      48&lt;/p&gt;

&lt;p&gt;ä¸“ä¸šåŒ–ä¸è´¸æ˜“ï¼š&lt;/p&gt;</content><author><name></name></author><summary type="html">Principal of Economics ç¬¬ä¸€ç¯‡ å¯¼è¨€ ç»æµå­¦åå¤§åŸç† äººä»¬é¢ä¸´æƒè¡¡å–èˆ æŸç§ä¸œè¥¿çš„æˆæœ¬æ˜¯ä¸ºäº†å¾—åˆ°å®ƒæ‰€æ”¾å¼ƒçš„ä¸œè¥¿ ç†æ€§äººè€ƒè™‘è¾¹é™…é‡ äººä»¬ä¼šå¯¹æ¿€åŠ±åšå‡ºååº” è´¸æ˜“å¯ä»¥ä½¿æ¯ä¸ªäººçš„çŠ¶å†µéƒ½å˜å¾—æ›´å¥½ å¸‚åœºé€šå¸¸æ˜¯ç»„ç»‡ç»æµæ´»åŠ¨çš„ä¸€ç§å¥½æ–¹æ³• æ”¿åºœæœ‰æ—¶å¯ä»¥æ”¹å–„å¸‚åœºç»“æœ ä¸€å›½çš„ç”Ÿæ´»æ°´å¹³å–å†³äºå®ƒç”Ÿäº§ç‰©å“ä¸æœåŠ¡çš„èƒ½åŠ› å½“æ”¿åºœå‘å‘è¡Œäº†è¿‡å¤šè´§å¸æ—¶ï¼Œç‰©ä»·ä¸Šå‡ ç¤¾ä¼šé¢ä¸´é€šè´§è†¨èƒ€ä¸å¤±ä¸šä¹‹é—´çš„çŸ­æœŸæƒè¡¡å–èˆ</summary></entry><entry><title type="html">lifelong problem</title><link href="http://localhost:5555/blog/lifelong_problem/" rel="alternate" type="text/html" title="lifelong problem" /><published>2023-03-28T00:00:00+08:00</published><updated>2023-03-28T00:00:00+08:00</updated><id>http://localhost:5555/blog/lifelong_problem</id><content type="html" xml:base="http://localhost:5555/blog/lifelong_problem/">&lt;h3 id=&quot;a-lifelong-learning-approach-tomobile-robot-navigation&quot;&gt;A Lifelong Learning Approach toMobile Robot Navigation&lt;/h3&gt;
&lt;p&gt;Motivation:
Mobile robots often encounter new environments, and classic methods require experts to tune parameters based on experience or intuition. Current learning-based models, such as Deep Reinforcement Learning (DRL), suffer from bad generalization and tend to forget past parameters when training in a new environment. To solve this â€œlifelong learningâ€ problem, three categories of approaches have been proposed: 1) regularization to prevent learned weights from deviating too much from old weights; 2) training generative models to recover old data for joint optimization; and 3) adopting dynamic network architectures for learning more tasks.&lt;/p&gt;

&lt;p&gt;Intuition:
Catastrophic forgetting occurs when old knowledge is overwritten by new learning, resulting in difficulty navigating in specific scenarios. Therefore, the paper proposes a Lifelong Learning for Navigation (LLfN) framework that learns an auxiliary planner Ï€Î¸ to assist a classical planner Ï€0 only in navigating difficult instances.&lt;/p&gt;

&lt;p&gt;Comparison:
Itâ€™s worth noting that LLfN has similarities to transfer learning, but it also considers backward transfer - how learning the current task can maintain or improve the performance of old tasks.&lt;/p&gt;

&lt;p&gt;Method:
The paper uses Gradient Episodic Memory (GEM) from category one, which allows new experiences to improve performance on old tasks while imposing constraints on past jobs. 
&lt;img src=&quot;http://localhost:5555/assets/images/A_Lifelong_Learning_Approach_to/equation1.png&quot; alt=&quot;&quot; /&gt; &lt;br /&gt;
GEM optimizes the loss function with L(Ï€Î¸,X) = E(s,a)âˆ¼X||Ï€Î¸(s)âˆ’a||2.
&lt;img src=&quot;http://localhost:5555/assets/images/A_Lifelong_Learning_Approach_to/LLfm1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Training:
The paper proposes an approach for training mobile robots to navigate using a combination of classical planning and lifelong learning. The method is based on an initialized beginning policy, which can be random or based on a classical planner such as DWA, to explore the environment. The robotâ€™s experience is stored in a memory buffer, which is used to retain information about past experiences and improve performance on previous tasks.&lt;/p&gt;

&lt;p&gt;To discriminate the performance of actions in a particular state, the paper proposes an algorithm based on heuristics extracted from DWA, which evaluates the quality of each action and selects the best one. If the performance is below a threshold, the best actions from the memory buffer are retrieved and used to update the buffer of the current environment. The policy is then optimized using GEM, which improves the robotâ€™s performance on past tasks while allowing it to learn new ones.&lt;/p&gt;

&lt;p&gt;Prediction:
The proposed approach uses a combination of the beginning policy and the current policy to determine navigation. The beginning policy is used to explore the environment and collect experience, while the current policy is optimized using GEM to improve performance on past tasks and learn new ones. The resulting policy is then used to navigate the robot through the environment.&lt;/p&gt;

&lt;h3 id=&quot;lifelong-roboticre-inforcement-learning-by-retaining-experiences&quot;&gt;LIFELONG ROBOTICRE INFORCEMENT LEARNING BY RETAINING EXPERIENCES&lt;/h3&gt;
&lt;p&gt;motivation: the standard multi-task reinforcement learning paradigm, which requires datacollection from each task in round-robin fashion, can often be unrealistic for embodied agents such as physical robots.
the main challenge is forward transfer. The traditional method is weight transfer which may discard important information from the past experiencesthat  are  relevant  to  future  tasks.
The author use the experience replay method. We measure the similarity between the past samples and the current taskâ€™s transition dynamics to determinewhich samples to transfer in the online fine-tuning phase.&lt;/p&gt;

&lt;p&gt;relatedï¼š
fine-tuning method: Efficient adaptation for end-to-end vision-based robotic manipulation. 2020
continual learning: Knowledge transfer in deep actor-critic reinforcement learning 2020
domain adapation: DARC 2021&lt;/p&gt;

&lt;h3 id=&quot;towards-continual-reinforcement-learninga-review-and-perspectives&quot;&gt;Towards Continual Reinforcement Learning:A Review and Perspectives&lt;/h3&gt;
&lt;p&gt;continual learning approaches:
1.explicit knowledge retention
1). parameter storage based
I.use of shared latent components
II.provide the representation of networks trained on previous task
    curse of dimension / storage requirement
    -&amp;gt;consider a single representation
III.store a prior about the extent of past usage of each parameters(popular)
context information
2).distillation based
3).rehearsal based
    experience replay
        improvement:learn to compress experiences during learning&lt;/p&gt;

&lt;p&gt;2.leveraging shared structure
continual learning agents should reuse aspects of the solutions to previously solved subproblems through function composition by abstracting relevant meaningful information in the form of abstract concepts or skills.
1).modularity and composition focused
 Compositional generalization canbe understood as leveraging prior experience to solve compositional perturbations of priorproblems.
 decomposite the task into several modules, train in seperate modules and combine them together.
2).state abstractions focused
State abstraction (or aggregation) is central to the idea of capturing common structurewithin various tasks and potentially facilitating positive forward transfer across related tasks.
I. One such abstraction is state abstractions based on the PAC framework.
II. can be preserved while learning abstractionsis the underlying reward and transition model,
3).skill focused
4).goal focused
5).auxiliaary tasks focused&lt;/p&gt;

&lt;p&gt;3.learning to learn
1).context detection
2).learning to adapt
the agent attempts to learn to alter its own optimization process based on historical success and failure.&lt;/p&gt;</content><author><name></name></author><summary type="html">A Lifelong Learning Approach toMobile Robot Navigation Motivation: Mobile robots often encounter new environments, and classic methods require experts to tune parameters based on experience or intuition. Current learning-based models, such as Deep Reinforcement Learning (DRL), suffer from bad generalization and tend to forget past parameters when training in a new environment. To solve this â€œlifelong learningâ€ problem, three categories of approaches have been proposed: 1) regularization to prevent learned weights from deviating too much from old weights; 2) training generative models to recover old data for joint optimization; and 3) adopting dynamic network architectures for learning more tasks.</summary></entry><entry><title type="html">icra_2022</title><link href="http://localhost:5555/blog/rl_icra_2022/" rel="alternate" type="text/html" title="icra_2022" /><published>2023-03-20T00:00:00+08:00</published><updated>2023-03-20T00:00:00+08:00</updated><id>http://localhost:5555/blog/rl_icra_2022</id><content type="html" xml:base="http://localhost:5555/blog/rl_icra_2022/">&lt;ol&gt;
  &lt;li&gt;Nearest-Neighbor-Based Collision Avoidance for Quadrotors Via Reinforcement Learning&lt;/li&gt;
  &lt;li&gt;Learning Multi-Task Transferable Rewards Via Variational Inverse Reinforcement Learning&lt;/li&gt;
  &lt;li&gt;Weakly Supervised Disentangled Representation for Goal-Conditioned Reinforcement Learning&lt;/li&gt;
  &lt;li&gt;AMI: Adaptive Motion Imitation Algorithm Based on Deep Reinforcement Learning&lt;/li&gt;
  &lt;li&gt;Human-Following and -Guiding in Crowded Environments Using Semantic Deep-Reinforcement-Learning for Mobile Service Robots&lt;/li&gt;
  &lt;li&gt;Decentralized Ride-Sharing of Shared Autonomous Vehicles Using Graph Neural Network-Based Reinforcement Learning&lt;/li&gt;
  &lt;li&gt;Excavation Reinforcement Learning Using Geometric Representation&lt;/li&gt;
  &lt;li&gt;Value Learning from Trajectory Optimization and Sobolev Descent: A Step Toward Reinforcement Learning with Superlinear Convergence Properties&lt;/li&gt;
  &lt;li&gt;Reinforcement Learning with Evolutionary Trajectory Generator: A General Approach for Quadrupedal Locomotion&lt;/li&gt;
  &lt;li&gt;Unified Data Collection for Visual-Inertial Calibration Via Deep Reinforcement Learning&lt;/li&gt;
  &lt;li&gt;Relative Distributed Formation and Obstacle Avoidance with Multi-Agent Reinforcement Learning&lt;/li&gt;
  &lt;li&gt;Toward Expedited Impedance Tuning of a Robotic Prosthesis for Personalized Gait Assistance by Reinforcement Learning Control&lt;/li&gt;
  &lt;li&gt;Offline Learning of Counterfactual Perception As Prediction for Real-World Robotic Reinforcement Learning&lt;/li&gt;
  &lt;li&gt;A Deep Reinforcement Learning Environment for Particle Robot Navigation and Object Manipulation&lt;/li&gt;
  &lt;li&gt;Confidence-Based Robot Navigation under Sensor Occlusion with Deep Reinforcement Learning&lt;/li&gt;
  &lt;li&gt;Deep Reinforcement Learning for Next-Best-View Planning in Agricultural Applications&lt;/li&gt;
  &lt;li&gt;Real-Robot Deep Reinforcement Learning: Improving Trajectory Tracking of Flexible-Joint Manipulator with Reference Correction&lt;/li&gt;
  &lt;li&gt;Discovering Synergies for Robot Manipulationwith Multi-Task Reinforcement Learning&lt;/li&gt;
  &lt;li&gt;Personalized Car Following for Autonomous Driving with Inverse Reinforcement Learning&lt;/li&gt;
  &lt;li&gt;Closed-Loop Dynamic Control of a Soft Manipulator Using Deep Reinforcement Learning
21.Seeking Visual Discomfort: Curiosity-Driven Representations for Reinforcement Learning&lt;/li&gt;
&lt;/ol&gt;</content><author><name></name></author><summary type="html">Nearest-Neighbor-Based Collision Avoidance for Quadrotors Via Reinforcement Learning Learning Multi-Task Transferable Rewards Via Variational Inverse Reinforcement Learning Weakly Supervised Disentangled Representation for Goal-Conditioned Reinforcement Learning AMI: Adaptive Motion Imitation Algorithm Based on Deep Reinforcement Learning Human-Following and -Guiding in Crowded Environments Using Semantic Deep-Reinforcement-Learning for Mobile Service Robots Decentralized Ride-Sharing of Shared Autonomous Vehicles Using Graph Neural Network-Based Reinforcement Learning Excavation Reinforcement Learning Using Geometric Representation Value Learning from Trajectory Optimization and Sobolev Descent: A Step Toward Reinforcement Learning with Superlinear Convergence Properties Reinforcement Learning with Evolutionary Trajectory Generator: A General Approach for Quadrupedal Locomotion Unified Data Collection for Visual-Inertial Calibration Via Deep Reinforcement Learning Relative Distributed Formation and Obstacle Avoidance with Multi-Agent Reinforcement Learning Toward Expedited Impedance Tuning of a Robotic Prosthesis for Personalized Gait Assistance by Reinforcement Learning Control Offline Learning of Counterfactual Perception As Prediction for Real-World Robotic Reinforcement Learning A Deep Reinforcement Learning Environment for Particle Robot Navigation and Object Manipulation Confidence-Based Robot Navigation under Sensor Occlusion with Deep Reinforcement Learning Deep Reinforcement Learning for Next-Best-View Planning in Agricultural Applications Real-Robot Deep Reinforcement Learning: Improving Trajectory Tracking of Flexible-Joint Manipulator with Reference Correction Discovering Synergies for Robot Manipulationwith Multi-Task Reinforcement Learning Personalized Car Following for Autonomous Driving with Inverse Reinforcement Learning Closed-Loop Dynamic Control of a Soft Manipulator Using Deep Reinforcement Learning 21.Seeking Visual Discomfort: Curiosity-Driven Representations for Reinforcement Learning</summary></entry><entry><title type="html">reinforce_hierachical_method</title><link href="http://localhost:5555/blog/rl-hierachical/" rel="alternate" type="text/html" title="reinforce_hierachical_method" /><published>2023-03-12T00:00:00+08:00</published><updated>2023-03-12T00:00:00+08:00</updated><id>http://localhost:5555/blog/rl-hierachical</id><content type="html" xml:base="http://localhost:5555/blog/rl-hierachical/">&lt;h3 id=&quot;a-deep-hierarchical-approach-to-lifelong-learning-in-minecraft&quot;&gt;A Deep Hierarchical Approach to Lifelong Learning in Minecraft&lt;/h3&gt;
&lt;p&gt;when encoutering lifelong problem, efficient retention and tranfer prior knowledge to the new task is necessary because of the curse of diemension. The process is divided into 3 stage:&lt;br /&gt;
1.retain previous knowledge &lt;br /&gt;
2.needs the ability to choose relevant prior knowledge for solving new tasks.&lt;br /&gt;
3.Ensures the effective and efficient interaction of the retention and transfer elements.&lt;/p&gt;

&lt;h3 id=&quot;policy-distillation&quot;&gt;Policy distillation&lt;/h3&gt;

&lt;p&gt;temperature_softmax&lt;br /&gt;
normal softmax: $\frac{e^(a_i)}{\sum_i e^(a_i)}$
temperature_softmax: 
eg.
softmax result:
[0.09003057317038046, 0.24472847105479767, 0.6652409557748219]
when t = 2ï¼Œequals [1/2,2/2,3/2],calculate softmax, getï¼š
[0.1863237232258476, 0.30719588571849843, 0.506480391055654]
when t = 0.5ï¼Œequals [1/0.5,2/0.5,3/0.5],calculate softmax, getï¼š
[0.015876239976466765, 0.11731042782619835, 0.8668133321973348]&lt;/p&gt;

&lt;p&gt;so it is obvious that when t is larger the result getting more smooth, get more chance to explore the low probability choices.&lt;/p&gt;

&lt;p&gt;so methods:
\tau = $\frac{\tau_0}{1+\log_{2}^{T}}$
T is the training times
so when the training progress, \tau decreases, making  the network become less smooth.&lt;/p&gt;

&lt;p&gt;option is like a intra-policy that works under the whole process&lt;/p&gt;

&lt;h3 id=&quot;the-option-critic-architecture&quot;&gt;The Option-Critic Architecture&lt;/h3&gt;
&lt;p&gt;The majority of the existing work has focused on finding subgoals and subsequently learning policies to achieve them. This method presents an alternative view, which blurs the line between the problem of discovering options from that of learning options(subgoals). We derive policy gradient theorems for options and propose a new option-critic architecture capable of learning both the internal policies and the termination conditions of options, in tandem with the policy over options, and without the need to provide any additional rewards or subgoals.&lt;/p&gt;

&lt;p&gt;option w&lt;/p&gt;</content><author><name></name></author><summary type="html">A Deep Hierarchical Approach to Lifelong Learning in Minecraft when encoutering lifelong problem, efficient retention and tranfer prior knowledge to the new task is necessary because of the curse of diemension. The process is divided into 3 stage: 1.retain previous knowledge 2.needs the ability to choose relevant prior knowledge for solving new tasks. 3.Ensures the effective and efficient interaction of the retention and transfer elements.</summary></entry><entry><title type="html">Welcome to Raoâ€™s blog</title><link href="http://localhost:5555/blog/welcome-to-jekyll/" rel="alternate" type="text/html" title="Welcome to Raoâ€™s blog" /><published>2023-03-08T10:59:04+08:00</published><updated>2023-03-08T10:59:04+08:00</updated><id>http://localhost:5555/blog/welcome-to-jekyll</id><content type="html" xml:base="http://localhost:5555/blog/welcome-to-jekyll/">&lt;p&gt;Youâ€™ll find this post in your &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_posts&lt;/code&gt; directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jekyll serve&lt;/code&gt;, which launches a web server and auto-regenerates your site when a file is updated.&lt;/p&gt;

&lt;p&gt;To add new posts, simply add a file in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_posts&lt;/code&gt; directory that follows the convention &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;YYYY-MM-DD-name-of-post.ext&lt;/code&gt; and includes the necessary front matter. Take a look at the source for this post to get an idea about how it works.&lt;/p&gt;

&lt;p&gt;Jekyll also offers powerful support for code snippets:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;puts&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Hi, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;#{&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&apos;Tom&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#=&amp;gt; prints &apos;Hi, Tom&apos; to STDOUT.&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Check out the &lt;a href=&quot;https://jekyllrb.com/docs/home&quot;&gt;Jekyll docs&lt;/a&gt; for more info on how to get the most out of Jekyll. File all bugs/feature requests at &lt;a href=&quot;https://github.com/jekyll/jekyll&quot;&gt;Jekyllâ€™s GitHub repo&lt;/a&gt;. If you have questions, you can ask them on &lt;a href=&quot;https://talk.jekyllrb.com/&quot;&gt;Jekyll Talk&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html">Youâ€™ll find this post in your _posts directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run jekyll serve, which launches a web server and auto-regenerates your site when a file is updated.</summary></entry><entry><title type="html">reinforce_navigation</title><link href="http://localhost:5555/blog/rl-vnavi/" rel="alternate" type="text/html" title="reinforce_navigation" /><published>2023-03-08T00:00:00+08:00</published><updated>2023-03-08T00:00:00+08:00</updated><id>http://localhost:5555/blog/rl-vnavi</id><content type="html" xml:base="http://localhost:5555/blog/rl-vnavi/">&lt;h1 id=&quot;a-survey-on-visual-navigation-for-artificial-agents-with-deep-reinforcement-learning&quot;&gt;A survey on Visual Navigation for Artificial Agents with deep reinforcement learning&lt;/h1&gt;

&lt;p&gt;the paper divide rl algorithms into three categories: 
1.Value-based methods&lt;br /&gt;
2.Policy-based methods&lt;br /&gt;
3.Actor-critic methods.&lt;/p&gt;

&lt;p&gt;and list four most used network/algorithms:
 1.deep Q-network&lt;br /&gt;
 2.Deep determinstic policy gradient&lt;br /&gt;
 3.Asynchronous advantage actor-critic(A3C)&lt;br /&gt;
 4.Proximal policy optimization&lt;/p&gt;

&lt;p&gt;5 visual navigation methods:&lt;br /&gt;
 1.direct drl vNavigation&lt;br /&gt;
  In detail, image depth information is conducive for theagent to avoid obstacles, and the closed-loop detection can beused for efficient exploration and spatial reasoning.&lt;/p&gt;

&lt;p&gt;A method is A3C with auxiliary tasks[59], but auxiliary task mainly depends on human selection. And then Zhu[63] fed the target images into actor-critic NN. In the area of path-planning, [69] proposed a differentiabe path planning method: value iteration network(VIN). [71] reconstructed VIN as arecursive convolutional network.
 2.hierarchical methods:
  To solve dimentional diaster, hierachical methods decomposes vnavigation into subproblems.  &lt;br /&gt;
  1) Hierarchical abstract machines&lt;br /&gt;
take final gaol into some subgoals and solve subgoals first. But it relies on manually constructing subgoals. &lt;br /&gt;
  &lt;img src=&quot;http://localhost:5555/assets/images/ham.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;
  then comes to [73]~[77]&lt;br /&gt;
  2).option methods&lt;br /&gt;
  big area!!&lt;br /&gt;
 3.multi-task drl vNavigation
  Traditional DRL agents can navigate well in one domain but will perform poorly in other unseen domains, which meanstraditional DRL navigation lacks transferability.&lt;br /&gt;
  1). distillation method&lt;br /&gt;
  the original distillation model is the Rusuet al.[83]. Then convolution was introduced to solve the same data distribution problem&lt;/p&gt;

&lt;p&gt;2). progress model&lt;/p&gt;

&lt;p&gt;4.memory-inferene drl vNavigation
 1). replay buffer
 normal replay buffer in DQN&lt;br /&gt;
 hierarchical prioritized experience replay to selectively choose experiences&lt;br /&gt;
 store the world model in the buffer&lt;br /&gt;
 SoRB graph search to decomposed target into a sequence of easier subgoals to solve sparse problem&lt;br /&gt;
 2).memory networks&lt;br /&gt;
 hese proposed architectures storerecent observations into their memory and retrieve relevantmemory based on the temporal context, common networks include MQN/RMQN/FRMQN
 3).episodic memory&lt;/p&gt;

&lt;p&gt;5).vision-and-language (VLN)
 agents follow the natural languague instructions to navigate.&lt;/p&gt;

&lt;p&gt;explore unseen environments by imitating its past and good decisions.&lt;/p&gt;

&lt;p&gt;5.challenge:
1).data inefficiency
when inputs are high-dimensional, model usually needs a large number of samples./ sparse rewards-&amp;gt; poor convergence and long training time.
2).poor generalization
 deep network are hard to tranfer
     ) partial observable
6.oppotunities
1).policy hierarchy
decomposes into subproblems
2).meta learning
training from a small amount of data
3).memory
 draw on previous experience in similar conditions from memory
4).multi-modal fusion&lt;/p&gt;

&lt;h3 id=&quot;target-driven-visual-navigation-in-indoor-scenesusing-deep-reinforcement-learning&quot;&gt;Target-driven Visual Navigation in Indoor Scenesusing Deep Reinforcement Learning&lt;/h3&gt;
&lt;p&gt;this paper proposes two methods for solving two problems:1.lack of generalization capability to new goals 2.data inefficiency, require several episodes to converge.
Then comes to two solutions:1.actor-critic models include targets. 2. propose AI2-THOR framework provides episodes with high-quality 3D scene.&lt;/p&gt;

&lt;p&gt;However,general DRL approaches (e.g., [2], [3]) are designed tolearn a policy that depends only on the current state, andthe goal is implicitly embedded in the model parameters.Hence, it is necessary to learn new model parameters for anew target. This is problematic since training DRL agents iscomputationally expensive.&lt;/p&gt;

&lt;p&gt;1.target generalization 2.scene generalization 3.real-world generalization&lt;/p&gt;

&lt;h3 id=&quot;value-iteration-networks&quot;&gt;Value Iteration Networks&lt;/h3&gt;
&lt;p&gt;novel differentiable approximation of the value-iteration algorithm&lt;/p&gt;

&lt;h3 id=&quot;end-to-end-training-of-deep-visuomotor-policies&quot;&gt;End-to-End Training of Deep Visuomotor Policies&lt;/h3&gt;</content><author><name></name></author><summary type="html">A survey on Visual Navigation for Artificial Agents with deep reinforcement learning</summary></entry><entry><title type="html">reinforce_basic</title><link href="http://localhost:5555/blog/rl_basic/" rel="alternate" type="text/html" title="reinforce_basic" /><published>2023-03-07T00:00:00+08:00</published><updated>2023-03-07T00:00:00+08:00</updated><id>http://localhost:5555/blog/rl_basic</id><content type="html" xml:base="http://localhost:5555/blog/rl_basic/">&lt;!--

# reinforce_basic
 
## ç›®å½•
+ [ç¬¬ä¸€éƒ¨åˆ†](#partI)
+ [ç¬¬äºŒéƒ¨åˆ†](#partII)
+ [ç¬¬ä¸‰éƒ¨åˆ†](#partIII)
 07 Mar 2023
----------------------------------
 --&gt;

&lt;h1 id=&quot;a3c&quot;&gt;A3C&lt;/h1&gt;

&lt;h1 id=&quot;a2c&quot;&gt;A2C&lt;/h1&gt;
&lt;p&gt;add one coordinator to&lt;/p&gt;

&lt;h1 id=&quot;dpg&quot;&gt;DPG&lt;/h1&gt;
&lt;p&gt;some notes:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;deterministic policy gradient is the limitting case, as policy variance tends to zero.&lt;/li&gt;
  &lt;li&gt;policy gradient integrates over both state and action spaces, whereas in the determinstic case it only integrates over the state space.&lt;/li&gt;
  &lt;li&gt;build a actor-critic algorithms&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;action space A and policy needs to be continuous
Q:
ä¸ºä»€ä¹ˆ approximation é‚£é‡Œi)è¦æ±‚çº¿æ€§&lt;/p&gt;

&lt;h1 id=&quot;maddpg&quot;&gt;MADDPG&lt;/h1&gt;
&lt;p&gt;centralizd training(critic,allowing the policy to use extra information to ease training )/decentralized execution(actor)
thus proposed a simple extension of actor-critic policy gradient methods where the critic is augmented with extra information about the policy of the other agents&lt;/p&gt;

&lt;p&gt;for training:&lt;/p&gt;

&lt;p&gt;$ E=mc^2 $
\(\beta\)&lt;/p&gt;

&lt;h1 id=&quot;trpo&quot;&gt;TRPO&lt;/h1&gt;
&lt;p&gt;trust region policy optimization(TRPO). &lt;br /&gt;
 Normal policy gradient method suffers from the unconvergence because of the big difference one batch. The paper prove that minimizing a certain surrogate objective funtion guarantees policy improvement with non-trivial step sizes.&lt;/p&gt;

&lt;p&gt;###Approximation:
$Î·(Ï€) =Es0,a0,â€¦[âˆâˆ‘t=0Î³tr(st)]$ Î·(Ï€)is the reward function that starting from the initial state.
the expected return of another policy ÌƒÏ€ can be expressed as
Î·( ÌƒÏ€) =Î·(Ï€) +Es0,a0,Â·Â·Â·âˆ¼ ÌƒÏ€[âˆâˆ‘t=0Î³tAÏ€(st,at)]
define LÏ€( ÌƒÏ€) =Î·(Ï€) +âˆ‘sÏÏ€(s)âˆ‘a ÌƒÏ€(a|s)AÏ€(s,a).
note that the visitation frequency Ï ÌƒÏ€(s) is approximated as ÏÏ€(s). It leads to some kinds of error.
the paper derive the lower bound 
Î·(Ï€new)â‰¥LÏ€old(Ï€new)âˆ’2Î³(1âˆ’Î³)2Î±2where= maxsâˆ£âˆ£Eaâˆ¼Ï€â€²(a|s)[AÏ€(s,a)]âˆ£âˆ£ by KL divergence.&lt;/p&gt;

&lt;h1 id=&quot;ppo&quot;&gt;PPO&lt;/h1&gt;
&lt;p&gt;the gradient estimator is g=Ë†Et[âˆ‡Î¸logÏ€Î¸(at|st)Ë†At]
it is equal to differential the loss function:
LPG(Î¸) =Ë†Et[logÏ€Î¸(at|st)Ë†At]&lt;/p&gt;</content><author><name></name></author><summary type="html">&amp;lt;!â€“</summary></entry></feed>